
# ğŸ§  OpenIE Task 1 â€” Data Collection (Neuroscience of Mental Health)

> **KDD Lab / Open Information Extraction (OIE) Testbed**  
> Foundation layer for discovering, normalizing, and scoring social-media content in the domain of mental-health neuroscience.

---

## Overview

This repository implements **Task 1: Data Collection** for a multi-task Open IE research system. It provides a **modular, reproducible pipeline** that:

1) defines **domain seeds** (problem statements & example phrases),  
2) generates **platform-specific search queries**,  
3) calls **platform APIs** (YouTube, Reddit),  
4) **normalizes** heterogeneous results to a common schema,  
5) **scores** previews with domain-tuned heuristics, and  
6) creates a **small labeled developer set** and **evaluates thresholds** (PR / ROC / F1) to select a production **KEEP** cutoff.

This data layer feeds downstream tasks such as information extraction (entities/relations), knowledge-graph building, ranking, and evaluation.

---

## Research Focus (reframed)

We target the **neuroscience of mental health** across public discourse and scientific context, including:

- Clinical vs **self-diagnosis** of **ASD, ADHD, AuDHD**  
- **Informal signs/symptoms** and **pop-psychology coping** strategies  
- **Neurodivergent identity & community** narratives  
- **Ethical debates** surrounding diagnostics & treatments  
- Emerging research on **DREADDs**, **genetic pathways**, **detection methods**, **diagnostic procedures**, and **pharmaceuticals** potentially affecting neurodivergent conditions

The pipeline seeks content that connects public conversation with contemporary scientific themes, surfacing high-value material that naÃ¯ve keyword search would miss.

---

## Relationship to Other Tasks

| Task | What it does | How it uses Task 1 |
|---|---|---|
| **Task 1 â€“ Data Collection (this repo)** | Seeds â†’ queries â†’ API fetch â†’ normalize â†’ score â†’ label & calibrate | Produces high-quality, scored previews and labeled sets |
| **Task 2 â€“ Ground Truth & Annotation** | Curates and scales labeled datasets | Bootstraps from `sample_for_labeling.py` and thresholding results |
| **Task 3 â€“ Information Extraction** | NER, relation/claim extraction from posts/videos | Consumes normalized text/snippets/transcripts from Task 1 |
| **Task 4 â€“ Knowledge Graphs** | Ontology integration of extracted facts | Links to Task 1 source URLs and metadata |
| **Task 5 â€“ Ranking & Evaluation** | Learned ranking, metrics, and ablations | Uses Task 1 thresholds and labels for training/validation |

---

## Repository Structure


```bash
OpenIE_Task_1_Data_Collection
â”œâ”€â”€ .env
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ .github
â”œâ”€â”€ .gitignore
â”œâ”€â”€ bash_scratch.txt
â”œâ”€â”€ cli
â”‚   â”œâ”€â”€ eval_thresholds.py
â”‚   â”œâ”€â”€ gen_queries.py
â”‚   â”œâ”€â”€ sample_for_labeling.py
â”‚   â””â”€â”€ score_previews.py
â”œâ”€â”€ dev_init.ps1
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ documents_for_chat_context
â”œâ”€â”€ OpenIE_Task_1_Data_Collection.ini
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scripts
â”‚   â””â”€â”€ demo_fetch_and_score.py
â”œâ”€â”€ src
â”‚   â””â”€â”€ oie_search
â”‚       â”œâ”€â”€ apis
â”‚       â”‚   â”œâ”€â”€ reddit.py
â”‚       â”‚   â””â”€â”€ youtube.py
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ db
â”‚       â”‚   â”œâ”€â”€ mongo_init.js
â”‚       â”‚   â”œâ”€â”€ mongo_runner.py
â”‚       â”‚   â”œâ”€â”€ postgres_runner.py
â”‚       â”‚   â”œâ”€â”€ schema_postgres.sql
â”‚       â”‚   â”œâ”€â”€ seed_mongo.js
â”‚       â”‚   â”œâ”€â”€ seed_postgres.sql
â”‚       â”‚   â””â”€â”€ __init__.py
â”‚       â”œâ”€â”€ digestors.py
â”‚       â”œâ”€â”€ docs
â”‚       â”‚   â””â”€â”€ schema.md
â”‚       â”œâ”€â”€ pipelines
â”‚       â”‚   â”œâ”€â”€ generate_queries.py
â”‚       â”‚   â””â”€â”€ preview_intake.py
â”‚       â”œâ”€â”€ prompts.py
â”‚       â”œâ”€â”€ query_generator.py
â”‚       â”œâ”€â”€ scoring.py
â”‚       â”œâ”€â”€ utils
â”‚       â”‚   â””â”€â”€ logging.py
â”‚       â””â”€â”€ __init__.py
â””â”€â”€ tests
    â”œâ”€â”€ test_digestors.py
    â”œâ”€â”€ test_query_generator.py
    â””â”€â”€ test_scoring.py
```





---

## Quick Start

### 0) Install & start services
```bash
pip install -r requirements.txt

# Start Postgres + Mongo locally
docker-compose up -d
# (Optional convenience on Windows)
powershell -ExecutionPolicy Bypass -File dev_init.ps1



1) Configure environment

Create .env (see Environment Variables below) with DB connection and API keys:

cp .env.example .env   # if you keep an example, else create manually

2) Generate queries from seeds
python cli/gen_queries.py --backend postgres --limit 10

3) Fetch previews (YouTube/Reddit), normalize & score

Use scripts/demo_fetch_and_score.py for a quick end-to-end smoke test, or integrate your own fetcher using src/oie_search/apis/*.

Persist fetched items to the previews table/collection, then:

python cli/score_previews.py --backend postgres --batch-size 200 --limit 5000 --dump-csv top_previews.csv

4) Build a small ground truth & calibrate threshold
python cli/sample_for_labeling.py --backend postgres --n 100 --out labels_devset.csv
# Manually label gold_keep as 1/0 in the CSV
python cli/eval_thresholds.py --labels labels_devset.csv --report thresholds_report.csv
# Update KEEP_MIN in scoring (or ini) per recommendation

Components & Functions (what each file does)
cli/

gen_queries.py

parse_args() â€” CLI flags (--backend, --platforms, --limit, etc.).

main() â€” loads backend via db.get_backend(), iterates seeds, calls query_generator.generate_queries_for_platform(), persists to search_queries.

score_previews.py

parse_args(), _setup_logger() â€” CLI & logging.

Batch loop: backend.list_unscored_previews() â†’ digestors.normalize_preview(platform, raw) â†’ scoring.score_preview(seed, normalized) â†’ backend.save_preview_scores(rows).

Optional per-seed leaderboard CSV via --dump-csv.

sample_for_labeling.py

_pg_fetch_scored() / _mongo_fetch_scored() â€” pull a large random pool of scored previews.

_stratify_by_score(rows, n, bins=8) â€” stratified sampling across score range for balanced labels.

main() â€” writes labels_devset.csv with fields to annotate + placeholders (notes, gold_keep).

eval_thresholds.py

_load(labels_csv) â€” reads score and gold_keep from CSV.

_sweep_thresholds(y_score, y_true) â€” computes precision/recall/F1 across score thresholds.

main() â€” prints ROC-AUC, PR-AUC, per-threshold metrics, and recommends KEEP_MIN (max F1; tie-break by precision â†’ higher threshold); optionally writes thresholds_report.csv.

src/oie_search/

config.py

PLATFORMS_PRIORITY â€” ordering of platforms to target first.

QueryGenConfig â€” pydantic config (include author flag, time windows, language priority).

DEFAULT_QCFG â€” default query-generation config object.

query_generator.py

normalize_text(s) â€” trims & collapses whitespace.

top_k_terms(text, k=8) â€” lightweight term frequency extractor (drops digits/stopwords).

build_phrase_candidates(seed) â€” uses seed title + first sentences from description/transcript/ocr/body; deduplicates to ~6 phrases.

generate_queries_for_platform(seed, platform, config=None) â€” core per-platform templates (YouTube, Reddit, federated, podcasts, Threads/Twitter/X, generic); returns {precise, broad, hashtag_phrase}; can prepend from:<author> if configured.

prompts.py

PREVIEW_ANALYZER_PROMPT â€” LLM rubric template for 0â€“100 â€œdownload interestâ€ scoring (semantic/lexical/hashtag/media/recency/engagement) with a compact JSON output format (used in future learned-ranking extensions).

digestors.py

normalize_preview(platform, raw) â€” converts native API items to a common schema:

{
  "platform": "youtube|reddit|...",
  "url": "...",
  "title": "...",
  "snippet": "...",
  "author": "...",
  "date": "ISO-8601",
  "hashtags": ["..."],
  "engagement": {"views": int?, "likes": int?, "comments": int?},
  "media": {"has_video": bool?, "duration_sec": int?},
  "raw": {...}
}


_normalize_youtube(raw) â€” merges search snippet + statistics + duration.

_normalize_reddit(raw) â€” uses public or OAuth fields to populate normalized schema.

featurize_preview(seed, normalized_preview) â€” optional shim (if you later split scoring into {features â†’ score}).

scoring.py

Constants: KEEP_MIN, TOPK_PER_SEED, weight caps (title/desc overlap, domain/novel terms, recency half-life, credibility, engagement caps).

_quick_text_cosine(a,b) â€” TF-IDF cosine between short texts.

score_preview(seed, preview) â€” heuristic ensemble: semantic similarity + lexical phrase hits + hashtag overlap + media/recency bonuses + engagement; returns {"score": float, "decision": "keep|consider|reject", "signals": {...}}.

apis/

youtube.py

search_videos(query, published_after=None, max_results=None, order="relevance")
Returns items that already combine snippet + statistics + content details (duration) and convenient fields (title, description, channelTitle, publishedAt, statistics, durationSec).

reddit.py

search_posts(query, sort="relevance", limit=25, oauth=False)
Supports public JSON endpoint (no OAuth; needs REDDIT_USER_AGENT) or OAuth (set REDDIT_CLIENT_ID/SECRET/USERNAME/PASSWORD) for better reliability.

db/

__init__.py

BaseBackend interface; concrete PostgresBackend, MongoBackend.

get_backend(name) â€” returns the requested backend (or by QUERY_BACKEND env).

Methods expected by CLIs:
list_seeds(limit), save_generated_queries(rows),
list_unscored_previews(batch_size, limit), save_preview_scores(rows),
(optional) get_seed(seed_id) if you choose to implement a join.

schema_postgres.sql â€” tables seeds_table, search_queries, previews, plus indexes/uniques.

seed_postgres.sql â€” 10 seed topics spanning diagnosis vs self-diagnosis, identity/ethics, DREADDs, genetics, screening, trials; includes example queries and a sample preview row.

postgres_runner.py / mongo_runner.py â€” examples for iterating seeds, generating queries, and persisting to search_queries.

mongo_init.js / seed_mongo.js â€” MongoDB bootstrap (mirror Postgres seed content if you use Mongo first-class).

pipelines/

generate_queries.py â€” reads QUERY_BACKEND and dispatches to Postgres/Mongo query generation.

preview_intake.py â€” example intake: normalize â†’ score â†’ return structured results (wires digestors + scoring).

docs/schema.md

Documents the normalized preview schema, key fields, and suggested DB column types.

utils/logging.py

Tiny logging helpers (formatters, levels) for consistent CLI output.

scripts/

demo_fetch_and_score.py

Minimal end-to-end test: runs a query through YouTube + Reddit clients, normalizes and scores previews, and prints top results (useful for sanity checks before full ingestion).

tests/

test_query_generator.py â€” validates phrase selection & platform templates.

test_digestors.py â€” validates normalization shape and critical fields.

test_scoring.py â€” validates scoring range, monotonicities of key signals.

Environment Variables

Define in .env:

# --- Database ---
POSTGRES_DSN="host=localhost dbname=oie user=postgres password=postgres"
MONGO_URI="mongodb://localhost:27017"
MONGO_DB="oie"
SEEDS_TABLE=seeds_table
SEARCH_QUERIES_TABLE=search_queries
PREVIEWS_TABLE=previews
QUERY_BACKEND=postgres
LOG_LEVEL=INFO

# --- YouTube ---
YOUTUBE_API_KEY=YOUR_KEY
YOUTUBE_MAX_RESULTS=25

# --- Reddit ---
REDDIT_USER_AGENT=OpenIE/0.1 (by u/your_username)
# OAuth (optional but recommended)
REDDIT_CLIENT_ID=
REDDIT_CLIENT_SECRET=
REDDIT_USERNAME=
REDDIT_PASSWORD=


Scoring weights / thresholds: see OpenIE_Task_1_Data_Collection.ini (or edit constants in scoring.py).

Data Model (preview schema)

See src/oie_search/docs/schema.md for details. Core fields saved to DB/CSV:

{
  "seed_id": 123,
  "platform": "youtube|reddit|...",
  "url": "...",
  "title": "...",
  "snippet": "...",
  "author": "...",
  "date": "ISO-8601",
  "hashtags": ["..."],
  "engagement": {"views": int?, "likes": int?, "comments": int?},
  "media": {"has_video": bool?, "duration_sec": int?},
  "score": 0..100,
  "decision": "keep|consider|reject",
  "signals": {...}
}

Ground Truth & Thresholds (recommended practice)

Run scoring to populate score.

Export a balanced dev set across score bins:
python cli/sample_for_labeling.py --backend postgres --n 100 --out labels_devset.csv

Manually label gold_keep âˆˆ {0,1}.

Evaluate thresholds & select KEEP_MIN:
python cli/eval_thresholds.py --labels labels_devset.csv --report thresholds_report.csv

Update KEEP_MIN (in scoring.py or the ini file) based on the recommended threshold.

Troubleshooting

No API results â€” verify keys in .env and internet access; for Reddit public search you must set REDDIT_USER_AGENT.

Cannot save scores â€” ensure schema_postgres.sql has been applied; tables/collections exist and match field names.

Encoding issues â€” store text as UTF-8; truncate overly long fields at ingestion (see digestors.normalize_preview).

Roadmap

Add Mastodon/BlueSky API clients & normalizers.

Integrate LLM re-ranking using prompts.PREVIEW_ANALYZER_PROMPT.

Expand annotation schema beyond binary keep/not-keep.

Move from heuristics â†’ learned ranking (Task 5).
